{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify\n",
      "[nltk_data]     failed: unable to get local issuer certificate\n",
      "[nltk_data]     (_ssl.c:992)>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:992)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:992)>\n",
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:992)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import ngrams\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.metrics import ndcg_score\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from nltk.util import bigrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import string\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['note','auteur', 'avis','assureur', 'produit', 'type', 'date_publication', 'date_exp', 'avis_en', 'avis_cor', 'avis_cor_en']\n",
    "df = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unable to open file ~$avis_2_traduit.xlsx\n",
      "unable to open file ~$avis_1_traduit.xlsx\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"Traduction_avis_clients\"  # Specify the folder path\n",
    "\n",
    "# Get the current working directory\n",
    "script_directory = os.getcwd()\n",
    "\n",
    "# Create the full path to the folder\n",
    "folder_full_path = os.path.join(script_directory, folder_path)\n",
    "\n",
    "# Check if the folder exists\n",
    "if os.path.exists(folder_full_path) and os.path.isdir(folder_full_path):\n",
    "    # List all files in the folder\n",
    "    files = os.listdir(folder_full_path)\n",
    "\n",
    "    # Loop through each file\n",
    "    for file_name in files:\n",
    "        # Create the full path to the file\n",
    "        file_path = os.path.join(folder_full_path, file_name)\n",
    "\n",
    "        # Check if it's a file (not a subdirectory)\n",
    "        if os.path.isfile(file_path):\n",
    "            # Read the content of the file\n",
    "            try:\n",
    "                small_df=pd.read_excel(file_path)\n",
    "                try:\n",
    "                    df = pd.concat([df, small_df], ignore_index=True)\n",
    "                except:\n",
    "                    print('unable to read data from '+file_name)\n",
    "            except:\n",
    "                print(\"unable to open file \"+file_name)\n",
    "else:\n",
    "    print(f\"The specified folder '{folder_path}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['avis', 'avis_en'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(val):\n",
    "    \"\"\" Cette fonction permet de supprimer les lettres, nombres et ensuite on\n",
    "    supprimer les stopwords et les mots de taille inférieure à 2 \"\"\"\n",
    "    STOP_WORDS = stopwords.words('english')\n",
    "    STOP_WORDS.append(\"\")\n",
    "    regex = re.compile('([^\\s\\w]|_)+')\n",
    "    sentence = regex.sub('', val).lower()\n",
    "    sentence = sentence.split(\" \")\n",
    "\n",
    "    for word in list(sentence):\n",
    "        if (word in STOP_WORDS) | (len(word)<=2):\n",
    "            sentence.remove(word)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "## Cette tokenisation ne fonctionne pas très bien\n",
    "def text2TokenList(text):\n",
    "    # Utilisation de clean_sentence\n",
    "    cleaned_text = clean_sentence(text)\n",
    "    # Lemmatisation\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    token_list = [lemmatizer.lemmatize(token) for token in cleaned_text]\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2BigramList(text):\n",
    "    cleaned_text = clean_sentence(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    token_list = [lemmatizer.lemmatize(token) for token in cleaned_text]\n",
    "    bigram_list=[]\n",
    "    # Generate bigrams\n",
    "    bigram_list += [f'{token1}_{token2}' for token1, token2 in bigrams(token_list)]\n",
    "    \n",
    "    return bigram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['avis_en'].apply(text2TokenList)\n",
    "df['bigrams'] = df['avis_en'].apply(text2BigramList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['auteur', 'avis_cor', 'avis_cor_en'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34434, 10)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>note</th>\n",
       "      <th>avis</th>\n",
       "      <th>assureur</th>\n",
       "      <th>produit</th>\n",
       "      <th>type</th>\n",
       "      <th>date_publication</th>\n",
       "      <th>date_exp</th>\n",
       "      <th>avis_en</th>\n",
       "      <th>tokens</th>\n",
       "      <th>bigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>j'ai quitté mon ancien contrat d'assurance che...</td>\n",
       "      <td>Néoliane Santé</td>\n",
       "      <td>sante</td>\n",
       "      <td>test</td>\n",
       "      <td>12/01/2017</td>\n",
       "      <td>01/01/2017</td>\n",
       "      <td>I left my former insurance contract at General...</td>\n",
       "      <td>[left, former, insurance, contract, general, y...</td>\n",
       "      <td>[left_former, former_insurance, insurance_cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>j'ai souscrit à cette mutuelle l'année dernier...</td>\n",
       "      <td>Néoliane Santé</td>\n",
       "      <td>sante</td>\n",
       "      <td>test</td>\n",
       "      <td>09/01/2017</td>\n",
       "      <td>01/01/2017</td>\n",
       "      <td>I subscribed to this mutual a year last year a...</td>\n",
       "      <td>[subscribed, mutual, year, last, year, stayed,...</td>\n",
       "      <td>[subscribed_mutual, mutual_year, year_last, la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Impossible d'avoir le bon service , ils raccro...</td>\n",
       "      <td>Néoliane Santé</td>\n",
       "      <td>sante</td>\n",
       "      <td>test</td>\n",
       "      <td>24/11/2016</td>\n",
       "      <td>01/11/2016</td>\n",
       "      <td>Impossible to have the right service, they han...</td>\n",
       "      <td>[impossible, right, service, hang, nose, know,...</td>\n",
       "      <td>[impossible_right, right_service, service_hang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Génération est une mutuelle très chère pour un...</td>\n",
       "      <td>Génération</td>\n",
       "      <td>sante</td>\n",
       "      <td>test</td>\n",
       "      <td>09/11/2021</td>\n",
       "      <td>01/11/2021</td>\n",
       "      <td>Generation is a very expensive mutual for a re...</td>\n",
       "      <td>[generation, expensive, mutual, retiree, 150, ...</td>\n",
       "      <td>[generation_expensive, expensive_mutual, mutua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>je viens d apprendre que je suis radié... j ap...</td>\n",
       "      <td>Génération</td>\n",
       "      <td>sante</td>\n",
       "      <td>test</td>\n",
       "      <td>08/11/2021</td>\n",
       "      <td>01/11/2021</td>\n",
       "      <td>I just learned that I am struck off ... I call...</td>\n",
       "      <td>[learned, struck, call, adviser, give, answer,...</td>\n",
       "      <td>[learned_struck, struck_call, call_adviser, ad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  note                                               avis        assureur  \\\n",
       "0  NaN  j'ai quitté mon ancien contrat d'assurance che...  Néoliane Santé   \n",
       "1  NaN  j'ai souscrit à cette mutuelle l'année dernier...  Néoliane Santé   \n",
       "2  NaN  Impossible d'avoir le bon service , ils raccro...  Néoliane Santé   \n",
       "3  NaN  Génération est une mutuelle très chère pour un...      Génération   \n",
       "4  NaN  je viens d apprendre que je suis radié... j ap...      Génération   \n",
       "\n",
       "  produit  type date_publication    date_exp  \\\n",
       "0   sante  test       12/01/2017  01/01/2017   \n",
       "1   sante  test       09/01/2017  01/01/2017   \n",
       "2   sante  test       24/11/2016  01/11/2016   \n",
       "3   sante  test       09/11/2021  01/11/2021   \n",
       "4   sante  test       08/11/2021  01/11/2021   \n",
       "\n",
       "                                             avis_en  \\\n",
       "0  I left my former insurance contract at General...   \n",
       "1  I subscribed to this mutual a year last year a...   \n",
       "2  Impossible to have the right service, they han...   \n",
       "3  Generation is a very expensive mutual for a re...   \n",
       "4  I just learned that I am struck off ... I call...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [left, former, insurance, contract, general, y...   \n",
       "1  [subscribed, mutual, year, last, year, stayed,...   \n",
       "2  [impossible, right, service, hang, nose, know,...   \n",
       "3  [generation, expensive, mutual, retiree, 150, ...   \n",
       "4  [learned, struck, call, adviser, give, answer,...   \n",
       "\n",
       "                                             bigrams  \n",
       "0  [left_former, former_insurance, insurance_cont...  \n",
       "1  [subscribed_mutual, mutual_year, year_last, la...  \n",
       "2  [impossible_right, right_service, service_hang...  \n",
       "3  [generation_expensive, expensive_mutual, mutua...  \n",
       "4  [learned_struck, struck_call, call_adviser, ad...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['type'] == 'train']\n",
    "df_test=df[df['type'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('train_set.csv', index=False)\n",
    "df_test.to_csv('test_set.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Token: [('insurance', 17048), ('service', 10669), ('price', 10630), ('year', 9094), ('contract', 8245), ('satisfied', 7968), ('customer', 6148), ('month', 5851), ('good', 5116), ('time', 5031)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_tokens = [token for sublist in df['tokens'] for token in sublist]\n",
    "\n",
    "token_counts = Counter(all_tokens)\n",
    "\n",
    "most_common_token = token_counts.most_common(10)\n",
    "print(\"Most Common Token:\", most_common_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Bigram: [('satisfied_service', 2247), ('customer_service', 2166), ('direct_insurance', 1625), ('satisfied_price', 892), ('insurance_company', 675), ('service_price', 619), ('take_care', 605), ('recommend_insurance', 602), ('price_suit', 599), ('car_insurance', 579)]\n"
     ]
    }
   ],
   "source": [
    "all_bigrams = [bigram for sublist in df['bigrams'] for bigram in sublist]\n",
    "\n",
    "bigram_counts = Counter(all_bigrams)\n",
    "\n",
    "most_common_bigram = bigram_counts.most_common(10)\n",
    "print(\"Most Common Bigram:\", most_common_bigram)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
