{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import ngrams\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.metrics import ndcg_score\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from nltk.util import bigrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import string\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['note','auteur', 'avis','assureur', 'produit', 'type', 'date_publication', 'date_exp', 'avis_en', 'avis_cor', 'avis_cor_en']\n",
    "df = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unable to open file ~$avis_1_traduit.xlsx\n",
      "unable to open file ~$avis_2_traduit.xlsx\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"Traduction_avis_clients\"  # Specify the folder path\n",
    "\n",
    "# Get the current working directory\n",
    "script_directory = os.getcwd()\n",
    "\n",
    "# Create the full path to the folder\n",
    "folder_full_path = os.path.join(script_directory, folder_path)\n",
    "\n",
    "# Check if the folder exists\n",
    "if os.path.exists(folder_full_path) and os.path.isdir(folder_full_path):\n",
    "    # List all files in the folder\n",
    "    files = os.listdir(folder_full_path)\n",
    "\n",
    "    # Loop through each file\n",
    "    for file_name in files:\n",
    "        # Create the full path to the file\n",
    "        file_path = os.path.join(folder_full_path, file_name)\n",
    "\n",
    "        # Check if it's a file (not a subdirectory)\n",
    "        if os.path.isfile(file_path):\n",
    "            # Read the content of the file\n",
    "            try:\n",
    "                small_df=pd.read_excel(file_path)\n",
    "                try:\n",
    "                    df = pd.concat([df, small_df], ignore_index=True)\n",
    "                except:\n",
    "                    print('unable to read data from '+file_name)\n",
    "            except:\n",
    "                print(\"unable to open file \"+file_name)\n",
    "else:\n",
    "    print(f\"The specified folder '{folder_path}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34435 entries, 0 to 34434\n",
      "Data columns (total 11 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   note              24104 non-null  object\n",
      " 1   auteur            34434 non-null  object\n",
      " 2   avis              34435 non-null  object\n",
      " 3   assureur          34435 non-null  object\n",
      " 4   produit           34435 non-null  object\n",
      " 5   type              34435 non-null  object\n",
      " 6   date_publication  34435 non-null  object\n",
      " 7   date_exp          34435 non-null  object\n",
      " 8   avis_en           34434 non-null  object\n",
      " 9   avis_cor          435 non-null    object\n",
      " 10  avis_cor_en       431 non-null    object\n",
      "dtypes: object(11)\n",
      "memory usage: 2.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "note                10331\n",
       "auteur                  1\n",
       "avis                    0\n",
       "assureur                0\n",
       "produit                 0\n",
       "type                    0\n",
       "date_publication        0\n",
       "date_exp                0\n",
       "avis_en                 1\n",
       "avis_cor            34000\n",
       "avis_cor_en         34004\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['avis_en','auteur'], inplace=True)\n",
    "df = df.drop(['auteur', 'avis_cor', 'avis_cor_en'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_avis(val):\n",
    "    # Replacing \\n\n",
    "    val = val.replace('\\n','')\n",
    "    # Suppresson special caracters\n",
    "    regex = re.compile('([^\\s\\w]|_)+')\n",
    "    sentence = regex.sub('', val).lower()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"avis\"] = df[\"avis\"].apply(clean_avis)\n",
    "df[\"avis_en\"] = df[\"avis_en\"].apply(clean_avis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = stopwords.words('english')\n",
    "STOP_WORDS.append(\"\")\n",
    "STOP_WORDS.extend(stopwords.words('french'))\n",
    "\n",
    "def clean_sentence(val):\n",
    "    \"\"\" Cette fonction permet de supprimer les caractères spéciaux, les stop words et \n",
    "    les mots de taille inférieure à 2 \"\"\"  \n",
    "    # Remplacement des valeurs \\n\n",
    "    val = val.replace('\\n','')\n",
    "    # Suppression des caratères spéciaux\n",
    "    regex = re.compile('([^\\s\\w]|_)+')\n",
    "    sentence = regex.sub('', val).lower()\n",
    "    sentence = sentence.split(\" \")\n",
    "\n",
    "    for word in list(sentence):\n",
    "        if (word in STOP_WORDS) | (len(word)<=2):\n",
    "            sentence.remove(word)\n",
    "    return sentence\n",
    "\n",
    "def text2TokenList(text):\n",
    "    # Utilisation de clean_sentence\n",
    "    cleaned_text = clean_sentence(text)\n",
    "    # Lemmatisation\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    token_list = [lemmatizer.lemmatize(token) for token in cleaned_text]\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2BigramList(text):\n",
    "    cleaned_text = clean_sentence(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    token_list = [lemmatizer.lemmatize(token) for token in cleaned_text]\n",
    "    bigram_list=[]\n",
    "    # Generate bigrams\n",
    "    bigram_list += [f'{token1}_{token2}' for token1, token2 in bigrams(token_list)]\n",
    "    return bigram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English tokens/bigrams cleaning\n",
    "df['tokens_en'] = df['avis_en'].apply(text2TokenList)\n",
    "df['bigrams_en'] = df['avis_en'].apply(text2BigramList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# French tokens cleaning\n",
    "df['tokens_fr'] = df['avis'].apply(text2TokenList)\n",
    "df['bigrams_fr'] = df['avis'].apply(text2BigramList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "note                10331\n",
       "avis                    0\n",
       "assureur                0\n",
       "produit                 0\n",
       "type                    0\n",
       "date_publication        0\n",
       "date_exp                0\n",
       "avis_en                 0\n",
       "tokens_en               0\n",
       "bigrams_en              0\n",
       "tokens_fr               0\n",
       "bigrams_fr              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['type'] == 'train']\n",
    "df_test=df[df['type'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "note                0\n",
       "avis                0\n",
       "assureur            0\n",
       "produit             0\n",
       "type                0\n",
       "date_publication    0\n",
       "date_exp            0\n",
       "avis_en             0\n",
       "tokens_en           0\n",
       "bigrams_en          0\n",
       "tokens_fr           0\n",
       "bigrams_fr          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "note                10331\n",
      "avis                    0\n",
      "assureur                0\n",
      "produit                 0\n",
      "type                    0\n",
      "date_publication        0\n",
      "date_exp                0\n",
      "avis_en                 0\n",
      "tokens_en               0\n",
      "bigrams_en              0\n",
      "tokens_fr               0\n",
      "bigrams_fr              0\n",
      "dtype: int64\n",
      "(10331, 12)\n"
     ]
    }
   ],
   "source": [
    "print(df_test.isna().sum())\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.drop(['note'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['type'], axis=1)\n",
    "df_test = df_test.drop(['type'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('train_set.csv', index=False)\n",
    "df_test.to_csv('test_set.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Token (English reviews): [('insurance', 17187), ('service', 10739), ('price', 10672), ('year', 9128), ('contract', 8298), ('satisfied', 7985), ('customer', 6184), ('month', 5877), ('good', 5131), ('time', 5051), ('without', 4525), ('vehicle', 4507), ('car', 4068), ('well', 4032), ('take', 3779), ('recommend', 3777), ('file', 3776), ('phone', 3693), ('since', 3621), ('day', 3585)]\n"
     ]
    }
   ],
   "source": [
    "# Identifying the top 20 most common words in the dataframe\n",
    "from collections import Counter\n",
    "all_tokens = [token for sublist in df['tokens_en'] for token in sublist]\n",
    "token_counts = Counter(all_tokens)\n",
    "most_common_token = token_counts.most_common(20)\n",
    "print(\"Most Common Token (English reviews):\", most_common_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Bigram (English reviews): [('satisfied_service', 2259), ('customer_service', 2183), ('direct_insurance', 1652), ('satisfied_price', 893), ('insurance_company', 680), ('service_price', 628), ('recommend_insurance', 612), ('take_care', 606), ('price_suit', 600), ('car_insurance', 581)]\n"
     ]
    }
   ],
   "source": [
    "# Identifying the top 20 most common bigrams in the dataframe\n",
    "all_bigrams = [bigram for sublist in df['bigrams_en'] for bigram in sublist]\n",
    "bigram_counts = Counter(all_bigrams)\n",
    "most_common_bigram = bigram_counts.most_common(10)\n",
    "print(\"Most Common Bigram (English reviews):\", most_common_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Token (French reviews): [('assurance', 14124), ('très', 13257), ('jai', 12501), ('plus', 11749), ('service', 10769), ('prix', 8580), ('bien', 7177), ('contrat', 6981), ('cest', 6889), ('tout', 6781), ('depuis', 6749), ('cette', 6312), ('fait', 6197), ('mois', 6164), ('client', 6138), ('chez', 5893), ('satisfait', 5806), ('faire', 5413), ('sans', 5393), ('sinistre', 4832)]\n"
     ]
    }
   ],
   "source": [
    "# Identifying the top 20 most common words in the dataframe\n",
    "from collections import Counter\n",
    "all_tokens = [token for sublist in df['tokens_fr'] for token in sublist]\n",
    "token_counts = Counter(all_tokens)\n",
    "most_common_token = token_counts.most_common(20)\n",
    "print(\"Most Common Token (French reviews):\", most_common_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Bigram (French reviews) : [('cette_assurance', 2270), ('service_client', 2241), ('satisfait_service', 2009), ('direct_assurance', 1911), ('prise_charge', 1353), ('très_satisfait', 1026), ('très_bien', 1012), ('depuis_an', 936), ('depuis_plus', 822), ('satisfaite_service', 756)]\n"
     ]
    }
   ],
   "source": [
    "# Identifying the top 20 most common bigrams in the dataframe\n",
    "all_bigrams = [bigram for sublist in df['bigrams_fr'] for bigram in sublist]\n",
    "bigram_counts = Counter(all_bigrams)\n",
    "most_common_bigram = bigram_counts.most_common(10)\n",
    "print(\"Most Common Bigram (French reviews) :\", most_common_bigram)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
