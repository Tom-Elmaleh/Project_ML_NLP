{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>note</th>\n",
       "      <th>avis</th>\n",
       "      <th>assureur</th>\n",
       "      <th>produit</th>\n",
       "      <th>date_publication</th>\n",
       "      <th>date_exp</th>\n",
       "      <th>avis_en</th>\n",
       "      <th>tokens_en</th>\n",
       "      <th>bigrams_en</th>\n",
       "      <th>tokens_fr</th>\n",
       "      <th>bigrams_fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>la personne au téléphone était clair et sympat...</td>\n",
       "      <td>L'olivier Assurance</td>\n",
       "      <td>auto</td>\n",
       "      <td>06/10/2021</td>\n",
       "      <td>01/10/2021</td>\n",
       "      <td>the person on the phone was clear and friendly...</td>\n",
       "      <td>['person', 'phone', 'clear', 'friendly', 'expl...</td>\n",
       "      <td>['person_phone', 'phone_clear', 'clear_friendl...</td>\n",
       "      <td>['personne', 'téléphone', 'clair', 'sympathiqu...</td>\n",
       "      <td>['personne_téléphone', 'téléphone_clair', 'cla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>satisfaitréactivité simplicité prix attractif ...</td>\n",
       "      <td>APRIL Moto</td>\n",
       "      <td>moto</td>\n",
       "      <td>09/07/2021</td>\n",
       "      <td>01/07/2021</td>\n",
       "      <td>satisfiedreactivity simplicity attractive pric...</td>\n",
       "      <td>['satisfiedreactivity', 'simplicity', 'attract...</td>\n",
       "      <td>['satisfiedreactivity_simplicity', 'simplicity...</td>\n",
       "      <td>['satisfaitréactivité', 'simplicité', 'prix', ...</td>\n",
       "      <td>['satisfaitréactivité_simplicité', 'simplicité...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   note                                               avis  \\\n",
       "0     4  la personne au téléphone était clair et sympat...   \n",
       "1     4  satisfaitréactivité simplicité prix attractif ...   \n",
       "\n",
       "              assureur produit date_publication    date_exp  \\\n",
       "0  L'olivier Assurance    auto       06/10/2021  01/10/2021   \n",
       "1           APRIL Moto    moto       09/07/2021  01/07/2021   \n",
       "\n",
       "                                             avis_en  \\\n",
       "0  the person on the phone was clear and friendly...   \n",
       "1  satisfiedreactivity simplicity attractive pric...   \n",
       "\n",
       "                                           tokens_en  \\\n",
       "0  ['person', 'phone', 'clear', 'friendly', 'expl...   \n",
       "1  ['satisfiedreactivity', 'simplicity', 'attract...   \n",
       "\n",
       "                                          bigrams_en  \\\n",
       "0  ['person_phone', 'phone_clear', 'clear_friendl...   \n",
       "1  ['satisfiedreactivity_simplicity', 'simplicity...   \n",
       "\n",
       "                                           tokens_fr  \\\n",
       "0  ['personne', 'téléphone', 'clair', 'sympathiqu...   \n",
       "1  ['satisfaitréactivité', 'simplicité', 'prix', ...   \n",
       "\n",
       "                                          bigrams_fr  \n",
       "0  ['personne_téléphone', 'téléphone_clair', 'cla...  \n",
       "1  ['satisfaitréactivité_simplicité', 'simplicité...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =pd.read_csv('train_set.csv',index_col=0)\n",
    "df['note'] = df['note'].apply(lambda x: int(x))\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Différents preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the text with lemmatization\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words_list = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return \" \".join(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the text with stemming\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text)    \n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the text without stemming and lemmatizing\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text) \n",
    "    toks = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF_IDF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5098527276498652\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.64      0.74      0.68      1444\n",
      "           2       0.36      0.37      0.37       716\n",
      "           3       0.32      0.22      0.26       665\n",
      "           4       0.45      0.41      0.43       999\n",
      "           5       0.54      0.58      0.56       997\n",
      "\n",
      "    accuracy                           0.51      4821\n",
      "   macro avg       0.46      0.46      0.46      4821\n",
      "weighted avg       0.49      0.51      0.50      4821\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['avis_en'], df['note'], test_size=0.2, random_state=42)\n",
    "# Preprocessing\n",
    "X_train_pre = X_train.apply(preprocess)\n",
    "X_test_pre = X_test.apply(preprocess)\n",
    "# TF IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_pre)\n",
    "X_test_tfidf = vectorizer.transform(X_test_pre)\n",
    "# Entrainement modèle\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(class_weight='balanced')\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "predictions = model.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4662933001451981\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.47      0.94      0.62      1444\n",
      "           2       0.34      0.08      0.13       716\n",
      "           3       0.39      0.05      0.09       665\n",
      "           4       0.45      0.33      0.38       999\n",
      "           5       0.51      0.48      0.49       997\n",
      "\n",
      "    accuracy                           0.47      4821\n",
      "   macro avg       0.43      0.37      0.34      4821\n",
      "weighted avg       0.44      0.47      0.40      4821\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['avis_en'], df['note'], test_size=0.2, random_state=42)\n",
    "# Preprocessing\n",
    "X_train_pre = X_train.apply(preprocess)\n",
    "X_test_pre = X_test.apply(preprocess)\n",
    "# Ajouter les bigrams\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2,2))\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_pre)\n",
    "X_test_tfidf = vectorizer.transform(X_test_pre)\n",
    "# Entrainement modèle\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(class_weight='balanced')\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "predictions = model.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LTSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional,Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "26/26 [==============================] - 12s 65ms/step - loss: 1.0441 - accuracy: 0.5332 - val_loss: 1.2043 - val_accuracy: 0.4828\n",
      "Epoch 2/50\n",
      "26/26 [==============================] - 0s 19ms/step - loss: 1.0239 - accuracy: 0.5477 - val_loss: 1.2098 - val_accuracy: 0.4820\n",
      "Epoch 3/50\n",
      "26/26 [==============================] - 0s 18ms/step - loss: 1.0159 - accuracy: 0.5547 - val_loss: 1.2287 - val_accuracy: 0.4844\n",
      "Epoch 4/50\n",
      "26/26 [==============================] - 1s 23ms/step - loss: 1.0032 - accuracy: 0.5559 - val_loss: 1.2484 - val_accuracy: 0.4836\n",
      "Epoch 5/50\n",
      "26/26 [==============================] - 0s 19ms/step - loss: 0.9965 - accuracy: 0.5564 - val_loss: 1.2687 - val_accuracy: 0.4688\n",
      "Epoch 6/50\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 0.9886 - accuracy: 0.5639 - val_loss: 1.2612 - val_accuracy: 0.4672\n",
      "Epoch 7/50\n",
      "26/26 [==============================] - 0s 18ms/step - loss: 0.9743 - accuracy: 0.5730 - val_loss: 1.2702 - val_accuracy: 0.4828\n",
      "Epoch 8/50\n",
      "26/26 [==============================] - 0s 18ms/step - loss: 0.9654 - accuracy: 0.5697 - val_loss: 1.3025 - val_accuracy: 0.4672\n",
      "Epoch 9/50\n",
      "26/26 [==============================] - 0s 19ms/step - loss: 0.9590 - accuracy: 0.5793 - val_loss: 1.3068 - val_accuracy: 0.4750\n",
      "Epoch 10/50\n",
      "26/26 [==============================] - 1s 20ms/step - loss: 0.9474 - accuracy: 0.5887 - val_loss: 1.2972 - val_accuracy: 0.4664\n",
      "Epoch 11/50\n",
      "26/26 [==============================] - 1s 22ms/step - loss: 0.9369 - accuracy: 0.5906 - val_loss: 1.3183 - val_accuracy: 0.4633\n",
      "Epoch 12/50\n",
      "26/26 [==============================] - 1s 27ms/step - loss: 0.9254 - accuracy: 0.5895 - val_loss: 1.3405 - val_accuracy: 0.4711\n",
      "Epoch 13/50\n",
      "26/26 [==============================] - 1s 25ms/step - loss: 0.9191 - accuracy: 0.5969 - val_loss: 1.3484 - val_accuracy: 0.4625\n",
      "Epoch 14/50\n",
      "26/26 [==============================] - 1s 19ms/step - loss: 0.9066 - accuracy: 0.6023 - val_loss: 1.3915 - val_accuracy: 0.4680\n",
      "Epoch 15/50\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 0.9006 - accuracy: 0.5971 - val_loss: 1.3776 - val_accuracy: 0.4563\n",
      "Epoch 16/50\n",
      "26/26 [==============================] - 0s 18ms/step - loss: 0.8961 - accuracy: 0.6014 - val_loss: 1.3990 - val_accuracy: 0.4664\n",
      "Epoch 17/50\n",
      "26/26 [==============================] - 1s 20ms/step - loss: 0.8842 - accuracy: 0.6137 - val_loss: 1.4245 - val_accuracy: 0.4578\n",
      "Epoch 18/50\n",
      "26/26 [==============================] - 0s 18ms/step - loss: 0.8723 - accuracy: 0.6096 - val_loss: 1.4430 - val_accuracy: 0.4664\n",
      "Epoch 19/50\n",
      "26/26 [==============================] - 1s 24ms/step - loss: 0.8625 - accuracy: 0.6238 - val_loss: 1.4805 - val_accuracy: 0.4703\n",
      "Epoch 20/50\n",
      "26/26 [==============================] - 1s 19ms/step - loss: 0.8587 - accuracy: 0.6361 - val_loss: 1.4963 - val_accuracy: 0.4523\n",
      "Epoch 21/50\n",
      "26/26 [==============================] - 1s 20ms/step - loss: 0.8437 - accuracy: 0.6330 - val_loss: 1.5287 - val_accuracy: 0.4703\n",
      "Epoch 22/50\n",
      "26/26 [==============================] - 0s 19ms/step - loss: 0.8332 - accuracy: 0.6398 - val_loss: 1.5354 - val_accuracy: 0.4633\n",
      "Epoch 23/50\n",
      "26/26 [==============================] - 0s 18ms/step - loss: 0.8255 - accuracy: 0.6406 - val_loss: 1.5381 - val_accuracy: 0.4555\n",
      "Epoch 24/50\n",
      "26/26 [==============================] - 1s 21ms/step - loss: 0.8153 - accuracy: 0.6441 - val_loss: 1.5696 - val_accuracy: 0.4391\n",
      "Epoch 25/50\n",
      "26/26 [==============================] - 1s 25ms/step - loss: 0.8166 - accuracy: 0.6463 - val_loss: 1.5524 - val_accuracy: 0.4508\n",
      "Epoch 26/50\n",
      "26/26 [==============================] - 1s 20ms/step - loss: 0.8077 - accuracy: 0.6434 - val_loss: 1.6081 - val_accuracy: 0.4453\n",
      "Epoch 27/50\n",
      "26/26 [==============================] - 1s 24ms/step - loss: 0.7883 - accuracy: 0.6576 - val_loss: 1.6431 - val_accuracy: 0.4586\n",
      "Epoch 28/50\n",
      "26/26 [==============================] - 0s 18ms/step - loss: 0.7689 - accuracy: 0.6689 - val_loss: 1.6870 - val_accuracy: 0.4641\n",
      "Epoch 29/50\n",
      "26/26 [==============================] - 1s 25ms/step - loss: 0.7647 - accuracy: 0.6727 - val_loss: 1.7115 - val_accuracy: 0.4445\n",
      "Epoch 30/50\n",
      "26/26 [==============================] - 1s 23ms/step - loss: 0.7568 - accuracy: 0.6762 - val_loss: 1.7176 - val_accuracy: 0.4539\n",
      "Epoch 31/50\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 0.7598 - accuracy: 0.6824 - val_loss: 1.7889 - val_accuracy: 0.4625\n",
      "Epoch 32/50\n",
      "26/26 [==============================] - 0s 18ms/step - loss: 0.7508 - accuracy: 0.6740 - val_loss: 1.8446 - val_accuracy: 0.4688\n",
      "Epoch 33/50\n",
      "26/26 [==============================] - 0s 18ms/step - loss: 0.7332 - accuracy: 0.6838 - val_loss: 1.8459 - val_accuracy: 0.4508\n",
      "Epoch 34/50\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 0.7192 - accuracy: 0.6895 - val_loss: 1.9212 - val_accuracy: 0.4547\n",
      "Epoch 35/50\n",
      "26/26 [==============================] - 0s 18ms/step - loss: 0.7119 - accuracy: 0.6951 - val_loss: 1.9050 - val_accuracy: 0.4539\n",
      "Epoch 36/50\n",
      "26/26 [==============================] - 1s 23ms/step - loss: 0.7106 - accuracy: 0.6932 - val_loss: 1.8869 - val_accuracy: 0.4539\n",
      "Epoch 37/50\n",
      "26/26 [==============================] - 1s 21ms/step - loss: 0.6949 - accuracy: 0.6947 - val_loss: 1.9984 - val_accuracy: 0.4609\n",
      "Epoch 38/50\n",
      "26/26 [==============================] - 1s 21ms/step - loss: 0.6763 - accuracy: 0.7145 - val_loss: 2.0459 - val_accuracy: 0.4383\n",
      "Epoch 39/50\n",
      "26/26 [==============================] - 0s 17ms/step - loss: 0.6797 - accuracy: 0.7139 - val_loss: 2.0497 - val_accuracy: 0.4516\n",
      "Epoch 40/50\n",
      "26/26 [==============================] - 1s 19ms/step - loss: 0.6592 - accuracy: 0.7225 - val_loss: 2.0860 - val_accuracy: 0.4633\n",
      "Epoch 41/50\n",
      "26/26 [==============================] - 1s 24ms/step - loss: 0.6571 - accuracy: 0.7287 - val_loss: 2.0900 - val_accuracy: 0.4313\n",
      "Epoch 42/50\n",
      "26/26 [==============================] - 0s 18ms/step - loss: 0.6435 - accuracy: 0.7346 - val_loss: 2.1732 - val_accuracy: 0.4414\n",
      "Epoch 43/50\n",
      "26/26 [==============================] - 1s 25ms/step - loss: 0.6479 - accuracy: 0.7293 - val_loss: 2.1896 - val_accuracy: 0.4570\n",
      "Epoch 44/50\n",
      "26/26 [==============================] - 0s 18ms/step - loss: 0.6480 - accuracy: 0.7279 - val_loss: 2.2994 - val_accuracy: 0.4484\n",
      "Epoch 45/50\n",
      "26/26 [==============================] - 1s 26ms/step - loss: 0.6196 - accuracy: 0.7467 - val_loss: 2.3492 - val_accuracy: 0.4336\n",
      "Epoch 46/50\n",
      "26/26 [==============================] - 1s 24ms/step - loss: 0.6180 - accuracy: 0.7402 - val_loss: 2.2376 - val_accuracy: 0.4531\n",
      "Epoch 47/50\n",
      "26/26 [==============================] - 1s 21ms/step - loss: 0.6047 - accuracy: 0.7482 - val_loss: 2.3523 - val_accuracy: 0.4305\n",
      "Epoch 48/50\n",
      "26/26 [==============================] - 0s 18ms/step - loss: 0.5893 - accuracy: 0.7563 - val_loss: 2.4410 - val_accuracy: 0.4297\n",
      "Epoch 49/50\n",
      "26/26 [==============================] - 0s 18ms/step - loss: 0.5794 - accuracy: 0.7609 - val_loss: 2.4540 - val_accuracy: 0.4422\n",
      "Epoch 50/50\n",
      "26/26 [==============================] - 1s 21ms/step - loss: 0.5697 - accuracy: 0.7604 - val_loss: 2.5270 - val_accuracy: 0.4383\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.3706 - accuracy: 0.4238\n",
      "Accuracy: 0.42375001311302185\n"
     ]
    }
   ],
   "source": [
    "sample_df = df.sample(n=8000, random_state=42)\n",
    "# Separate the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sample_df['avis_en'],\n",
    "    sample_df['note'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "# Apply preprocessing to the training and testing data\n",
    "X_train_processed= X_train.apply(preprocess)\n",
    "X_test_processed = X_test.apply(preprocess)\n",
    "# Convert text to USE embeddings\n",
    "use = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "X_train_use = np.array(use(X_train_processed.tolist()))\n",
    "X_test_use = np.array(use(X_test_processed.tolist()))\n",
    "# Adjust labels to start from 0\n",
    "y_train_adjusted = y_train - 1\n",
    "y_test_adjusted = y_test - 1\n",
    "# LSTM model with USE embeddings\n",
    "model = Sequential()\n",
    "model.add(Reshape((1, X_train_use.shape[1]), input_shape=(X_train_use.shape[1],)))  \n",
    "model.add(LSTM(128, activation='relu', return_sequences=True))\n",
    "model.add(LSTM(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# Train model\n",
    "model.fit(X_train_use, y_train_adjusted, validation_split=0.2, epochs=50, batch_size=200, verbose=1)\n",
    "# Evaluate the model on the test set\n",
    "accuracy = model.evaluate(X_test_use, y_test_adjusted)[1]\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "52/52 [==============================] - ETA: 0s - loss: 1.5902 - accuracy: 0.3012\n",
      "Epoch 1: val_accuracy improved from -inf to 0.28906, saving model to best_model.h5\n",
      "52/52 [==============================] - 15s 65ms/step - loss: 1.5902 - accuracy: 0.3012 - val_loss: 1.5180 - val_accuracy: 0.2891 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "50/52 [===========================>..] - ETA: 0s - loss: 1.3495 - accuracy: 0.4246\n",
      "Epoch 2: val_accuracy improved from 0.28906 to 0.45156, saving model to best_model.h5\n",
      "52/52 [==============================] - 1s 23ms/step - loss: 1.3456 - accuracy: 0.4266 - val_loss: 1.2038 - val_accuracy: 0.4516 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "52/52 [==============================] - ETA: 0s - loss: 1.2224 - accuracy: 0.4549\n",
      "Epoch 3: val_accuracy improved from 0.45156 to 0.47344, saving model to best_model.h5\n",
      "52/52 [==============================] - 1s 23ms/step - loss: 1.2224 - accuracy: 0.4549 - val_loss: 1.1784 - val_accuracy: 0.4734 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "51/52 [============================>.] - ETA: 0s - loss: 1.1864 - accuracy: 0.4657\n",
      "Epoch 4: val_accuracy improved from 0.47344 to 0.48203, saving model to best_model.h5\n",
      "52/52 [==============================] - 2s 30ms/step - loss: 1.1864 - accuracy: 0.4658 - val_loss: 1.1746 - val_accuracy: 0.4820 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "50/52 [===========================>..] - ETA: 0s - loss: 1.1727 - accuracy: 0.4698\n",
      "Epoch 5: val_accuracy did not improve from 0.48203\n",
      "52/52 [==============================] - 1s 27ms/step - loss: 1.1731 - accuracy: 0.4691 - val_loss: 1.1684 - val_accuracy: 0.4789 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "50/52 [===========================>..] - ETA: 0s - loss: 1.1692 - accuracy: 0.4768\n",
      "Epoch 6: val_accuracy did not improve from 0.48203\n",
      "52/52 [==============================] - 1s 29ms/step - loss: 1.1678 - accuracy: 0.4781 - val_loss: 1.1691 - val_accuracy: 0.4773 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "50/52 [===========================>..] - ETA: 0s - loss: 1.1484 - accuracy: 0.4796\n",
      "Epoch 7: val_accuracy did not improve from 0.48203\n",
      "52/52 [==============================] - 1s 24ms/step - loss: 1.1542 - accuracy: 0.4793 - val_loss: 1.1701 - val_accuracy: 0.4797 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "51/52 [============================>.] - ETA: 0s - loss: 1.1402 - accuracy: 0.4824\n",
      "Epoch 8: val_accuracy improved from 0.48203 to 0.48672, saving model to best_model.h5\n",
      "52/52 [==============================] - 1s 26ms/step - loss: 1.1403 - accuracy: 0.4824 - val_loss: 1.1662 - val_accuracy: 0.4867 - lr: 2.0000e-04\n",
      "Epoch 9/200\n",
      "51/52 [============================>.] - ETA: 0s - loss: 1.1344 - accuracy: 0.4849\n",
      "Epoch 9: val_accuracy did not improve from 0.48672\n",
      "52/52 [==============================] - 1s 27ms/step - loss: 1.1357 - accuracy: 0.4850 - val_loss: 1.1675 - val_accuracy: 0.4867 - lr: 2.0000e-04\n",
      "Epoch 10/200\n",
      "50/52 [===========================>..] - ETA: 0s - loss: 1.1409 - accuracy: 0.4810\n",
      "Epoch 10: val_accuracy did not improve from 0.48672\n",
      "52/52 [==============================] - 1s 25ms/step - loss: 1.1391 - accuracy: 0.4826 - val_loss: 1.1689 - val_accuracy: 0.4797 - lr: 2.0000e-04\n",
      "Epoch 11/200\n",
      "50/52 [===========================>..] - ETA: 0s - loss: 1.1359 - accuracy: 0.4732\n",
      "Epoch 11: val_accuracy did not improve from 0.48672\n",
      "52/52 [==============================] - 1s 28ms/step - loss: 1.1367 - accuracy: 0.4748 - val_loss: 1.1681 - val_accuracy: 0.4852 - lr: 2.0000e-04\n",
      "Epoch 12/200\n",
      "50/52 [===========================>..] - ETA: 0s - loss: 1.1332 - accuracy: 0.4854\n",
      "Epoch 12: val_accuracy did not improve from 0.48672\n",
      "52/52 [==============================] - 1s 28ms/step - loss: 1.1352 - accuracy: 0.4836 - val_loss: 1.1683 - val_accuracy: 0.4867 - lr: 1.0000e-04\n",
      "Epoch 13/200\n",
      "50/52 [===========================>..] - ETA: 0s - loss: 1.1401 - accuracy: 0.4840\n",
      "Epoch 13: val_accuracy did not improve from 0.48672\n",
      "52/52 [==============================] - 1s 24ms/step - loss: 1.1395 - accuracy: 0.4832 - val_loss: 1.1679 - val_accuracy: 0.4867 - lr: 1.0000e-04\n",
      "Epoch 14/200\n",
      "50/52 [===========================>..] - ETA: 0s - loss: 1.1273 - accuracy: 0.4830\n",
      "Epoch 14: val_accuracy improved from 0.48672 to 0.48828, saving model to best_model.h5\n",
      "52/52 [==============================] - 2s 35ms/step - loss: 1.1270 - accuracy: 0.4855 - val_loss: 1.1700 - val_accuracy: 0.4883 - lr: 1.0000e-04\n",
      "Epoch 15/200\n",
      "52/52 [==============================] - ETA: 0s - loss: 1.1258 - accuracy: 0.4812\n",
      "Epoch 15: val_accuracy did not improve from 0.48828\n",
      "52/52 [==============================] - 2s 33ms/step - loss: 1.1258 - accuracy: 0.4812 - val_loss: 1.1715 - val_accuracy: 0.4852 - lr: 1.0000e-04\n",
      "Epoch 16/200\n",
      "51/52 [============================>.] - ETA: 0s - loss: 1.1262 - accuracy: 0.4886\n",
      "Epoch 16: val_accuracy did not improve from 0.48828\n",
      "52/52 [==============================] - 2s 30ms/step - loss: 1.1262 - accuracy: 0.4887 - val_loss: 1.1708 - val_accuracy: 0.4844 - lr: 1.0000e-04\n",
      "Epoch 17/200\n",
      "50/52 [===========================>..] - ETA: 0s - loss: 1.1253 - accuracy: 0.4920\n",
      "Epoch 17: val_accuracy did not improve from 0.48828\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 1.1275 - accuracy: 0.4914 - val_loss: 1.1722 - val_accuracy: 0.4859 - lr: 1.0000e-04\n",
      "Epoch 18/200\n",
      "51/52 [============================>.] - ETA: 0s - loss: 1.1268 - accuracy: 0.4863\n",
      "Epoch 18: val_accuracy did not improve from 0.48828\n",
      "52/52 [==============================] - 1s 27ms/step - loss: 1.1262 - accuracy: 0.4865 - val_loss: 1.1715 - val_accuracy: 0.4859 - lr: 1.0000e-04\n",
      "Epoch 19/200\n",
      "50/52 [===========================>..] - ETA: 0s - loss: 1.1286 - accuracy: 0.4864\n",
      "Epoch 19: val_accuracy did not improve from 0.48828\n",
      "52/52 [==============================] - 2s 30ms/step - loss: 1.1281 - accuracy: 0.4869 - val_loss: 1.1721 - val_accuracy: 0.4859 - lr: 1.0000e-04\n",
      "Epoch 20/200\n",
      "49/52 [===========================>..] - ETA: 0s - loss: 1.1249 - accuracy: 0.4988\n",
      "Epoch 20: val_accuracy did not improve from 0.48828\n",
      "52/52 [==============================] - 1s 23ms/step - loss: 1.1246 - accuracy: 0.4969 - val_loss: 1.1723 - val_accuracy: 0.4852 - lr: 1.0000e-04\n",
      "Epoch 21/200\n",
      "50/52 [===========================>..] - ETA: 0s - loss: 1.1235 - accuracy: 0.4844\n",
      "Epoch 21: val_accuracy did not improve from 0.48828\n",
      "52/52 [==============================] - 2s 29ms/step - loss: 1.1226 - accuracy: 0.4836 - val_loss: 1.1722 - val_accuracy: 0.4867 - lr: 1.0000e-04\n",
      "Epoch 22/200\n",
      "50/52 [===========================>..] - ETA: 0s - loss: 1.1264 - accuracy: 0.4846\n",
      "Epoch 22: val_accuracy did not improve from 0.48828\n",
      "52/52 [==============================] - 2s 30ms/step - loss: 1.1258 - accuracy: 0.4844 - val_loss: 1.1726 - val_accuracy: 0.4844 - lr: 1.0000e-04\n",
      "Epoch 23/200\n",
      "52/52 [==============================] - ETA: 0s - loss: 1.1163 - accuracy: 0.4930\n",
      "Epoch 23: val_accuracy did not improve from 0.48828\n",
      "52/52 [==============================] - 2s 30ms/step - loss: 1.1163 - accuracy: 0.4930 - val_loss: 1.1743 - val_accuracy: 0.4859 - lr: 1.0000e-04\n",
      "Epoch 24/200\n",
      "52/52 [==============================] - ETA: 0s - loss: 1.1179 - accuracy: 0.4896\n",
      "Epoch 24: val_accuracy did not improve from 0.48828\n",
      "52/52 [==============================] - 1s 25ms/step - loss: 1.1179 - accuracy: 0.4896 - val_loss: 1.1757 - val_accuracy: 0.4836 - lr: 1.0000e-04\n",
      "Epoch 25/200\n",
      "51/52 [============================>.] - ETA: 0s - loss: 1.1204 - accuracy: 0.4914\n",
      "Epoch 25: val_accuracy did not improve from 0.48828\n",
      "52/52 [==============================] - 1s 25ms/step - loss: 1.1205 - accuracy: 0.4914 - val_loss: 1.1743 - val_accuracy: 0.4875 - lr: 1.0000e-04\n",
      "Epoch 26/200\n",
      "51/52 [============================>.] - ETA: 0s - loss: 1.1169 - accuracy: 0.4973\n",
      "Epoch 26: val_accuracy did not improve from 0.48828\n",
      "52/52 [==============================] - 1s 23ms/step - loss: 1.1174 - accuracy: 0.4969 - val_loss: 1.1749 - val_accuracy: 0.4836 - lr: 1.0000e-04\n",
      "Epoch 27/200\n",
      "49/52 [===========================>..] - ETA: 0s - loss: 1.1238 - accuracy: 0.4863\n",
      "Epoch 27: val_accuracy did not improve from 0.48828\n",
      "52/52 [==============================] - 1s 21ms/step - loss: 1.1196 - accuracy: 0.4877 - val_loss: 1.1753 - val_accuracy: 0.4828 - lr: 1.0000e-04\n",
      "Epoch 28/200\n",
      "50/52 [===========================>..] - ETA: 0s - loss: 1.1167 - accuracy: 0.4964\n",
      "Epoch 28: val_accuracy did not improve from 0.48828\n",
      "52/52 [==============================] - 1s 24ms/step - loss: 1.1156 - accuracy: 0.4980 - val_loss: 1.1762 - val_accuracy: 0.4852 - lr: 1.0000e-04\n",
      "Epoch 29/200\n",
      "52/52 [==============================] - ETA: 0s - loss: 1.1112 - accuracy: 0.4969\n",
      "Epoch 29: val_accuracy did not improve from 0.48828\n",
      "52/52 [==============================] - 1s 29ms/step - loss: 1.1112 - accuracy: 0.4969 - val_loss: 1.1787 - val_accuracy: 0.4836 - lr: 1.0000e-04\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 1.1542 - accuracy: 0.4694\n",
      "Accuracy: 0.46937501430511475\n"
     ]
    }
   ],
   "source": [
    "# Separate the data into training and testing sets\n",
    "sample_df = df.sample(n=8000, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sample_df['avis_en'],\n",
    "    sample_df['note'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Apply preprocessing to the training and testing data\n",
    "X_train_processed = X_train.apply(preprocess)\n",
    "X_test_processed = X_test.apply(preprocess)\n",
    "\n",
    "# Load Universal Sentence Encoder\n",
    "use = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# Convert text to USE embeddings\n",
    "X_train_use = use(X_train_processed.tolist())\n",
    "X_test_use = use(X_test_processed.tolist())\n",
    "\n",
    "y_train_adjusted = y_train-1\n",
    "y_test_adjusted = y_test-1\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='auto', verbose=0,patience=15)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=3, min_lr=0.0001)\n",
    "\n",
    "from tensorflow.keras.layers import Input, Reshape, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# LSTM model with USE embeddings\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(X_train_use.shape[1],)))\n",
    "model.add(Reshape((1, X_train_use.shape[1])))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))  # return_sequences must be True for stacking\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu'))  # another LSTM layer\n",
    "model.add(LSTM(32, activation='relu'))  # final LSTM layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5, activation='softmax'))  # Output layer with 5 neurons\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# Train the model\n",
    "history = model.fit(X_train_use, y_train_adjusted, validation_split=0.2, epochs=200, batch_size=100, verbose=1, callbacks=[es, mc, lr])\n",
    "# Evaluate the model on the test set\n",
    "accuracy = model.evaluate(X_test_use, y_test_adjusted)[1]\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\user\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': '2 stars', 'score': 0.341024249792099}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "# Load the sentiment analysis pipeline\n",
    "sentiment_pipeline = pipeline(\"text-classification\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "# Example usage\n",
    "review = str(df[\"avis_en\"].iloc[3])\n",
    "result = sentiment_pipeline(review)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
